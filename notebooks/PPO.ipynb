{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the /app/src directory to the Python path\n",
    "sys.path.append(os.path.abspath('../app/src'))\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym import Space\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import random\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Now try importing again\n",
    "from spark.data import loader\n",
    "from spark.data.models import Customer, Product, Category, Interaction, InteractionType\n",
    "from spark import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark.agent.environment import RecommendationEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Custom gym environment for product recommendations where states represent customer interactions\n",
    "actions are products;\n",
    "\n",
    "Each customer interaction is a state. In the step function, transition of states will be the same \n",
    "customer with the new interaction for a product. The transition ends when the customer made a purchase or session ends.\n",
    "\n",
    "The aim is to maximise rewards that can lead to a purchase.\n",
    "\n",
    "It is better to have a customer interaction to represent a state rather than a time series of steps leading to a purchase. \n",
    "The latter method may have an incomplete where customer exits the application. Also, even if the customer did not purchase,\n",
    "this information is still valuable for recommendations. Hence every state is a customer interaction.\n",
    "\n",
    "An addition meta data for each customer will be stored to understand the context of the user. For example, if a product is \n",
    "purchase many times, this may factor into preference of the states.\n",
    "---\n",
    "Personalization: By incorporating the user ID, the model can tailor recommendations specifically to individual users, allowing \n",
    "it to learn unique user preferences and behaviors over time.\n",
    "\n",
    "VS\n",
    "\n",
    "Overfitting: If the model learns too much from the user ID directly, it might overfit to individual user patterns, potentially \n",
    "missing out on broader trends that could be useful for all users.\n",
    "\n",
    "SOLUTION\n",
    "Use Embeddings: Instead of directly using user IDs, consider using an embedding layer that transforms the user ID into a dense vector representation. This approach reduces dimensionality while capturing user-specific features.\n",
    "\n",
    "Combine Features: Use user ID embeddings in conjunction with other features like user demographics, interaction history, and product attributes. This can create a more holistic view of user preferences.\n",
    "\n",
    "Regularization: Implement techniques like dropout or weight regularization to mitigate overfitting when using user IDs or their embeddings.\n",
    "\n",
    "Batch Normalization: Use batch normalization to stabilize learning, especially if the user ID leads to a wide range of outputs.\n",
    "----\n",
    "\"\"\"\n",
    "class RecommendationEnv(gym.Env):\n",
    "    def __init__(self, users:List[Customer], products:List[Product], top_k:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.users = users                  # list of users as states\n",
    "        self.products = products            # products as actions, potential recommendations\n",
    "        self.top_k = top_k                  # number of recommendations\n",
    "        self.user_idx = 0                   # index of users list, not user_id\n",
    "        self.current_step = 0               # step is also the interactions list index\n",
    "        self.categories = loader.load_categories()\n",
    "        \n",
    "        self.action_space = spaces.MultiDiscrete([len(products)] * 10) \n",
    "        \n",
    "        # number of customers as states\n",
    "        # states are derived from customer profiles and interactioms\n",
    "        # Users list will keep track of unique users\n",
    "        # States include subset of features including product, interaction, ratings, and time in one-hot-encoding format\n",
    "        # States exclude user_ids for policy network generalisation. But internal users list will be used as reference        \n",
    "        self.observation_space = spaces.Dict({\n",
    "            'pref_prod': spaces.Box(low=0, high=1, shape=(len(self.products),), dtype=np.float32),\n",
    "            'pref_cat': spaces.Box(low=0, high=1, shape=(len(self.categories),), dtype=np.float32),\n",
    "            'buys': spaces.Box(low=0, high=1, shape=(len(self.products),), dtype=np.uint8),\n",
    "            'views': spaces.Box(low=0, high=1, shape=(len(self.products),), dtype=np.uint8),\n",
    "            'likes': spaces.Box(low=0, high=1, shape=(len(self.products),), dtype=np.uint8),\n",
    "            'ratings': spaces.Box(low=0, high=1, shape=(len(self.products),), dtype=np.uint8),\n",
    "            'product': spaces.Box(low=0, high=1, shape=(len(self.products),), dtype=np.uint8),\n",
    "            'interaction': spaces.Box(low=0, high=1, shape=(len(list(InteractionType)),), dtype=np.uint8),\n",
    "            'rating': spaces.Discrete(6)\n",
    "            }) \n",
    "            ## add more features like time, ignored recommendtions, engagement etc\n",
    "        \n",
    "    def reset(self):\n",
    "        self.user_idx = np.random.randint(len(self.users)) # may run throught users one by one\n",
    "        user = self.users[self.user_idx]\n",
    "        self.current_step = 0\n",
    "        return self._get_observation(user) # get current user features as states\n",
    "\n",
    "    def step(self, rec_products):\n",
    "        \"\"\" randomly interacting with product to mimick real user unpredictable behavious \"\"\"\n",
    "        self.current_step += 1\n",
    "        user = self.users[self.user_idx]\n",
    "        \n",
    "        reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # simulate selected recommended product and interaction\n",
    "        seleted_pid, interaction_type = self._simulate_interaction(rec_products) # generate random interaction\n",
    "        # seleted_pid = random.choice(rec_products)\n",
    "        # interaction_type = random.choice(list(InteractionType))\n",
    "        \n",
    "        random_rating = 0\n",
    "        \n",
    "        if interaction_type == InteractionType.NONE:\n",
    "            reward = -1 # no interaction, customers not interested in recommendations\n",
    "        elif interaction_type ==  InteractionType.VIEW:\n",
    "            reward = 3\n",
    "        elif interaction_type ==  InteractionType.LIKE:\n",
    "            reward = 10\n",
    "        elif interaction_type ==  InteractionType.BUY:\n",
    "            reward = 50\n",
    "        elif interaction_type ==  InteractionType.RATE:\n",
    "            # generate rating, reward 1-2 is negative 3 neutral and 5 positive\n",
    "            random_rating = random.randint(0, 5)\n",
    "            reward = random_rating -1        \n",
    "        elif interaction_type ==  InteractionType.SESSION_START:\n",
    "            reward = 0\n",
    "        elif interaction_type ==  InteractionType.SESSION_CLOSE:\n",
    "            done = True\n",
    "            reward = 0 # TODO: check if engament is too short\n",
    "        else:\n",
    "            reward = 0\n",
    "        \n",
    "        # generate random interaction\n",
    "        new_interaction = Interaction(self.current_step, datetime.now(), user.idx, seleted_pid, interaction_type, random_rating)\n",
    "        # reward = self._calculate_reward(user, product)\n",
    "        \n",
    "        return self._update_observation(new_interaction), reward, done, {}\n",
    "\n",
    "    def _update_observation(self, interaction:Interaction):   \n",
    "        # update user data     \n",
    "        user = self.users[self.user_idx]   \n",
    "        pid = interaction.product_idx    \n",
    "         \n",
    "        if interaction == InteractionType.VIEW:\n",
    "            user.views[pid] += 1\n",
    "        elif interaction == InteractionType.LIKE:\n",
    "            user.likes[pid] += 1\n",
    "        elif interaction == InteractionType.BUY:\n",
    "            user.buys[pid] += 1\n",
    "        elif interaction == InteractionType.RATE:\n",
    "            user.rates[pid] = interaction.value\n",
    "          \n",
    "        # update observation based on new data  \n",
    "        obs = {\n",
    "                'pref_prod': self._get_product_preferences(user),\n",
    "                'pref_cat': self._get_category_preferences(user), \n",
    "                'buys': utils.normalise(user.buys),\n",
    "                'views': utils.normalise(user.views),\n",
    "                'likes': utils.normalise(user.likes),\n",
    "                'ratings': user.ratings,\n",
    "                'product': utils.one_hot_encode(pid, len(self.products)),\n",
    "                'interaction': self._get_interaction_observation(interaction),\n",
    "                'rating': interaction.value if interaction.type == InteractionType.RATE else 0 \n",
    "            }\n",
    "        \n",
    "        return obs\n",
    "\n",
    "    def _get_observation(self, user:Customer): \n",
    "        \n",
    "        obs = {\n",
    "                'pref_prod': self._get_product_preferences(user),\n",
    "                'pref_cat': self._get_category_preferences(user), \n",
    "                'buys': utils.normalise(user.buys),\n",
    "                'views': utils.normalise(user.views),\n",
    "                'likes': utils.normalise(user.likes),\n",
    "                'ratings': user.ratings,\n",
    "                'product': np.zeros(len(self.products)),\n",
    "                'interaction': np.zeros(len(list(InteractionType))),\n",
    "                'rating': 0 \n",
    "            }\n",
    "        \n",
    "        return obs\n",
    "        \n",
    "    def _get_interaction_observation(self, interaction:Interaction):\n",
    "        idx = list(InteractionType).index(interaction.type)\n",
    "        size = len(InteractionType)\n",
    "        \n",
    "        return utils.one_hot_encode(idx, size)\n",
    "    \n",
    "    # calculate preferences based on past interactions\n",
    "    def _get_product_preferences(self, user:Customer):\n",
    "        view_prefs = user.views / 20\n",
    "        purchase_prefs = user.buys\n",
    "        like_prefs = user.likes / 15\n",
    "\n",
    "        rating_prefs = user.ratings.copy()\n",
    "        rating_prefs[rating_prefs > 0] -= 2\n",
    "        \n",
    "        product_prefs = view_prefs + purchase_prefs + like_prefs+ rating_prefs\n",
    "        \n",
    "        return product_prefs    # calculate preferences based on past interactions\n",
    "    \n",
    "    def _get_category_preferences(self, user:Customer):\n",
    "        prod_prefs = self._get_product_preferences(user)\n",
    "        cat_prefs = np.zeros(len(self.categories), np.float32)\n",
    "        \n",
    "        for idx, prod_pref in enumerate(prod_prefs):\n",
    "            if prod_pref > 0:\n",
    "                product = self.products[idx]\n",
    "                cat_idx = product.category.idx\n",
    "                cat_prefs[cat_idx] += prod_pref # accumulation of fav products for this cat\n",
    "                # print(f\"added pf {prod_pref} to cat {cat_idx}\")\n",
    "                \n",
    "        cat_prefs = cat_prefs / 5 # reduce space     \n",
    "           \n",
    "        return cat_prefs\n",
    "\n",
    "    def _simulate_interaction(self, product_ids):        \n",
    "        user = self.users[self.user_idx]\n",
    "        product_list = []\n",
    "        \n",
    "        # simulate selection\n",
    "        num_products = len(product_ids)\n",
    "        prod_scores = np.zeros(num_products, np.uint8)\n",
    "        product_prefs = self._get_product_preferences(user)\n",
    "        category_prefs = self._get_category_preferences(user)\n",
    "        product_probs = np.full((num_products,), 1.0 / num_products) # equal probs by default\n",
    "        product_probs[-1] = 0.1 # lower ending epsidoe flag to encourage longer training\n",
    "        \n",
    "        for idx, pid in enumerate(product_ids):\n",
    "            product_list.append(self.products[pid]) # get the product objects\n",
    "            prod_scores[idx] = product_prefs[pid]\n",
    "            \n",
    "        # combining category prefs to calculate probabilities\n",
    "        for idx, product in enumerate(product_list):\n",
    "            cid = product.category.idx\n",
    "            prod_scores[idx] = category_prefs[cid] \n",
    "    \n",
    "        # Ensure the probabilities sum to 1 for a valid probability distribution\n",
    "        if np.argmax(prod_scores) > 0: # the product is in the preferences\n",
    "            product_probs = np.array(prod_scores) / sum(prod_scores)\n",
    "\n",
    "        # Randomly select a product based on the defined probabilities\n",
    "        selected_product_id = np.random.choice(product_ids, p=product_probs)\n",
    "        \n",
    "        # simulate interaction for the selected product\n",
    "        inter_types = list(InteractionType)\n",
    "        inter_scores = np.zeros(len(inter_types), np.uint8)\n",
    "        inter_probs = np.full((len(inter_types),), 1.0 / len(inter_types)) # equal probs by default\n",
    "        \n",
    "        \n",
    "        for idx, inter_type in enumerate(inter_types):\n",
    "            if inter_type == InteractionType.VIEW:\n",
    "                inter_scores[idx] = user.views[selected_product_id]\n",
    "            if inter_type == InteractionType.LIKE:\n",
    "                inter_scores[idx] = user.likes[selected_product_id]\n",
    "            if inter_type == InteractionType.BUY:\n",
    "                inter_scores[idx] = user.buys[selected_product_id]\n",
    "            if inter_type == InteractionType.RATE:\n",
    "                inter_scores[idx] = user.ratings[selected_product_id]\n",
    "        \n",
    "        if np.argmax(inter_scores) > 0:\n",
    "            inter_scores[inter_scores == 0] = 1 # default score for interaction that are 0\n",
    "            inter_probs = np.array(inter_scores) / sum(inter_scores)\n",
    "\n",
    "        # Randomly select a product based on the defined probabilities\n",
    "        selected_interaction_type = np.random.choice(inter_types, p=inter_probs)\n",
    "        \n",
    "        return selected_product_id, selected_interaction_type\n",
    "        \n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if hasattr(self, 'last_action'):\n",
    "            print(f\"Recommended Product ID (Last Action): {self.last_action}\")\n",
    "        else:\n",
    "            print(\"No product recommended yet.\")\n",
    "\n",
    "        # Optionally, print the reward received for the last action\n",
    "        if hasattr(self, 'last_reward'):\n",
    "            print(f\"Reward for Last Action: {self.last_reward}\")\n",
    "\n",
    "        print(\"-----\")\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<InteractionType.NONE: 'none'>,\n",
       " <InteractionType.VIEW: 'view'>,\n",
       " <InteractionType.LIKE: 'like'>,\n",
       " <InteractionType.BUY: 'buy'>,\n",
       " <InteractionType.RATE: 'rate'>,\n",
       " <InteractionType.EXIT: 'exit'>,\n",
       " <InteractionType.SESSION_START: 'session_start'>,\n",
       " <InteractionType.SESSION_CLOSE: 'session_close'>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(InteractionType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = loader.load_products()\n",
    "customers = loader.load_customers(include_interactions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for customer in customers:\n",
    "    customer.views = np.zeros(len(products), dtype=np.int8)\n",
    "    customer.likes = np.zeros(len(products), dtype=np.int8)\n",
    "    customer.buys = np.zeros(len(products), dtype=np.int8)\n",
    "    customer.ratings = np.zeros(len(products), dtype=np.int8)\n",
    "    for interaction in customer.interactions:\n",
    "        i_type = interaction.type.value    \n",
    "        product_idx = interaction.product_idx  \n",
    "        # print(f\"customer {customer.idx} interaction {type} product {product_idx}\")\n",
    "        if i_type == InteractionType.VIEW.value:\n",
    "            customer.views[product_idx] += 1\n",
    "            # print(f\"customer {customer.idx} view\", customer.views)\n",
    "        elif i_type == InteractionType.LIKE.value:\n",
    "            customer.likes[product_idx] += 1\n",
    "            # print(f\"customer {customer.idx} like\", customer.likes)\n",
    "        elif i_type == InteractionType.BUY.value:\n",
    "            customer.buys[product_idx] += 1\n",
    "            # print(f\"customer {customer.idx} buy\", customer.buys)\n",
    "        elif i_type == InteractionType.RATE.value:\n",
    "            customer.ratings[product_idx] = interaction.value\n",
    "            # print(f\"customer {customer.idx} rate\", customer.rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(InteractionType)\n",
    "cats = loader.load_categories()\n",
    "len(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<InteractionType.VIEW: 'view'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(list(InteractionType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasper/opt/anaconda3/lib/python3.9/site-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1000]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the env\n",
    "from gym.wrappers import FlattenObservation\n",
    "env = RecommendationEnv(customers, products, top_k=10)\n",
    "env.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# env.test_simiulate_interaction(products[:10])\n",
    "# env.observation_space.n\n",
    "pids = [product.idx for product in products]\n",
    "# env.test_simulate_interaction(pids[30:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<InteractionType.NONE: 'none'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# env = FlattenObservation(env)\n",
    "# env.observation_space\n",
    "random.choice(list(InteractionType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# print(type(env.observation_space))  # Output: <class 'int'>\n",
    "\n",
    "# def get_interaction_observation(interaction:Interaction):\n",
    "#     idx = list(InteractionType).index(interaction.type)\n",
    "#     size = len(InteractionType)\n",
    "#     # print(idx)\n",
    "    \n",
    "def get_interaction_observation(interaction:Interaction):\n",
    "        idx = list(InteractionType).index(interaction.type)\n",
    "        size = len(InteractionType)\n",
    "        return utils.one_hot_encode(idx, size)\n",
    "    \n",
    "get_interaction_observation(customers[0].interactions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasper/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/Users/jasper/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:155: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 10`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 10\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=10 and n_envs=1)\n",
      "  warnings.warn(\n",
      "/Users/jasper/opt/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7feaef3fde80>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from torch import nn\n",
    "\n",
    "ppo_log_dir = './logs'\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[dict(pi=[256, 256], vf=[256, 256])],\n",
    "    activation_fn=nn.ReLU,\n",
    ")\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"MultiInputPolicy\",\n",
    "    env=env,\n",
    "    verbose=0,\n",
    "    learning_rate=3e-4,\n",
    "    batch_size=64,\n",
    "    n_steps=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    ent_coef=0.0,\n",
    "    tensorboard_log=ppo_log_dir,\n",
    "    policy_kwargs=policy_kwargs,\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=30000)  # Adjust based on your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6011 (pid 17230), started 0:03:21 ago. (Use '!kill 17230' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b37b5a5c5748c6ea\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b37b5a5c5748c6ea\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6011;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir={ppo_log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended products: [262 224  48 279 209 279  99 113 241 106]\n",
      "Recommended products: [ 53 274  47  52 211 163  99 113 289 191]\n",
      "Recommended products: [162 213 272 263 286 154  99 255 114 191]\n",
      "Recommended products: [262  44  18 109 209 122  99 113 222 191]\n",
      "Recommended products: [262  48  33 279 232 253  99 113 188 131]\n",
      "Recommended products: [262 224 290 279  11 186  99  91 241 191]\n",
      "Recommended products: [262  22 151  52 291  17  99  91 290 191]\n",
      "Recommended products: [262 274 172 279  37 279  99 113 241 191]\n",
      "Recommended products: [262 277 272 284 286 110  99 113 263  69]\n",
      "Recommended products: [235 274  47 238 163  17  99 113 263 191]\n",
      "Recommended products: [262 135 272  59 264 279  99  91  22 191]\n",
      "Recommended products: [144 213 219 279 286  35  96  91 241 191]\n",
      "Recommended products: [236 274 293 279 286 202  99  91 111 191]\n",
      "Recommended products: [262  71  47 135 232 141  99 113  51 148]\n",
      "Recommended products: [262 277 140 115 286 279 167  35 171 191]\n",
      "Recommended products: [183  44 198 279 188 117  99 186 241 191]\n",
      "Recommended products: [262  78  47 279 287 206  99 269 259 191]\n",
      "Recommended products: [262 277  47 104 232 257  99  91 250 191]\n",
      "Recommended products: [142 274 233 279 286   3  99 204  50 191]\n",
      "Recommended products: [258  44 163 263  75  59  99 113 180 191]\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "for _ in range(20):  # Make 10 recommendations\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, _ = env.step(action)\n",
    "    print(\"Recommended products:\", action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
