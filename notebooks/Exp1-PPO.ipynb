{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1) PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Install necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"stable-baselines3[extra]>=2.0.0a4\"\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== Standard libraries for custom environment\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from typing import List\n",
    "from gymnasium import spaces\n",
    "from datetime import datetime\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#=== Custom utilities stored in .py files\n",
    "# Add the /app/src directory to the Python path\n",
    "sys.path.append(os.path.abspath('../app/src'))\n",
    "from spark.data import loader\n",
    "from spark.data.models import Customer, Product, Category, Interaction, InteractionType\n",
    "from spark import utils\n",
    "\n",
    "#=== Stable Baselines3 model libraries\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import A2C, PPO\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder, VecNormalize\n",
    "from stable_baselines3.common.preprocessing import get_flattened_obs_dim\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch\n",
    "from torch import nn\n",
    "import wandb\n",
    "\n",
    "#=== Print the versions of gymnasium and stable_baselines3 for debugging purposes \n",
    "print(f\"{gym.__version__=}\")\n",
    "print(f\"{stable_baselines3.__version__=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Experiment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project='spark'\n",
    "env_name = 'Spark'\n",
    "model_id='ppo'\n",
    "tb_log_name='PPO'\n",
    "label='main'\n",
    "inc=1\n",
    "run_name=f'{model_id}-{label}-{inc}'\n",
    "model_name_final = f\"{model_id}_model_final\"\n",
    "total_timesteps=int(3e6) # 3m timesteps for full training\n",
    "param_n_envs=1 # Added in case we wanted to allow multi-environment training via vector wrapper\n",
    "save_interval = total_timesteps/10\n",
    "\n",
    "# Weights and Biases callback configuration\n",
    "config = {\n",
    "    \"total_timesteps\": total_timesteps,\n",
    "    \"env_name\": env_name,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4) Define Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output directoies\n",
    "base_dir = '.'\n",
    "output_dir = os.path.join(base_dir, 'output')\n",
    "env_dir =  os.path.join(output_dir, project)\n",
    "logs_dir = os.path.join(env_dir, fr'logs/{run_name}')\n",
    "models_dir = os.path.join(env_dir, fr'models/{run_name}')\n",
    "\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "print(logs_dir)\n",
    "print(models_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4) Weights and Bias initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_API_KEY'] = 'please insert your API key here!'\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'Exp1-PPO'\n",
    "wandb.login()\n",
    "\n",
    "run = wandb.init(\n",
    "    project=project,\n",
    "    name=run_name,\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "    save_code=True,  # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Custom Callback utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbEvalCallback(EvalCallback, BaseCallback):\n",
    "    \"\"\"\n",
    "        Custom Callback that combines EvalCallback() and WandbEvalCallback().\n",
    "            - EvalCallback() independently evaluates the RL model and returns the \"Best performing\" one.\n",
    "            - WandbEvalCallback() ensures logs are uploaded to the Weights and Biases online server.\n",
    "\n",
    "        Sources:\n",
    "            - SB3 API Docs:         https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html\n",
    "            - SB3 codebase:         https://github.com/DLR-RM/stable-baselines3/blob/c62e9259db363bf32bd920405dbe83db94123271/stable_baselines3/common/callbacks.py\n",
    "            - Wandb codebase:       https://github.com/wandb/wandb/blob/main/wandb/integration/sb3/sb3.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 eval_env, \n",
    "                 wandb_run,\n",
    "                 save_interval: int = 10000,\n",
    "                 eval_freq: int = 10000, \n",
    "                 save_path: str = None, \n",
    "                 n_eval_episodes: int = 5, \n",
    "                 deterministic: bool = True, \n",
    "                 render: bool = False, \n",
    "                 verbose: int = 1):\n",
    "        \n",
    "        # Initialize EvalCallback\n",
    "        EvalCallback.__init__(self, \n",
    "                               eval_env=eval_env, \n",
    "                               eval_freq=eval_freq, \n",
    "                               best_model_save_path=save_path, \n",
    "                               n_eval_episodes=n_eval_episodes, \n",
    "                               deterministic=deterministic, \n",
    "                               render=render, \n",
    "                               verbose=verbose)\n",
    "\n",
    "        # Initialize BaseCallback (WandbCallback)\n",
    "        BaseCallback.__init__(self, verbose=verbose)\n",
    "        self.wandb_run = wandb_run\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.save_interval = save_interval\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Call the parent method to perform evaluation\n",
    "        super_result = super(WandbEvalCallback, self)._on_step()\n",
    "        \n",
    "        if super_result:\n",
    "            # Log metrics to wandb\n",
    "            self.wandb_run.log({\"mean_reward\": self.last_mean_reward, \"step\": self.num_timesteps})\n",
    "\n",
    "            # Log best model if it was updated\n",
    "            if self.save_path is not None and self.best_mean_reward is not None:\n",
    "                self.wandb_run.log({\"best_mean_reward\": self.best_mean_reward, \"step\": self.num_timesteps})\n",
    "\n",
    "        # Save the model every 'save_interval' steps\n",
    "        if self.num_timesteps % self.save_interval == 0:\n",
    "            interval_save=True\n",
    "            save_file = os.path.join(self.save_path, f'{model_id}_model_{self.num_timesteps}')\n",
    "            self.model.save(save_file)\n",
    "            if self.verbose > 0:\n",
    "                print(f'Saving model to {save_file}.zip')\n",
    "\n",
    "        return super_result or interval_save\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        # Log final metrics at the end of training\n",
    "        self.wandb_run.log({\"training_end\": True})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Custom Environment Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Product Recommendation Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Custom gym environment for product recommendations where states represent customer interactions\n",
    "actions are products;\n",
    "\n",
    "Each customer interaction is a state. In the step function, transition of states will be the same \n",
    "customer with the new interaction for a product. The transition ends when the customer made a purchase or session ends.\n",
    "\n",
    "The aim is to maximise rewards that can lead to a purchase.\n",
    "\n",
    "It is better to have a customer interaction to represent a state rather than a time series of steps leading to a purchase. \n",
    "The latter method may have an incomplete where customer exits the application. Also, even if the customer did not purchase,\n",
    "this information is still valuable for recommendations. Hence every state is a customer interaction.\n",
    "\n",
    "An addition meta data for each customer will be stored to understand the context of the user. For example, if a product is \n",
    "purchase many times, this may factor into preference of the states.\n",
    "---\n",
    "Personalization: By incorporating the user ID, the model can tailor recommendations specifically to individual users, allowing \n",
    "it to learn unique user preferences and behaviors over time.\n",
    "\n",
    "VS\n",
    "\n",
    "Overfitting: If the model learns too much from the user ID directly, it might overfit to individual user patterns, potentially \n",
    "missing out on broader trends that could be useful for all users.\n",
    "\n",
    "SOLUTION\n",
    "Use Embeddings: Instead of directly using user IDs, consider using an embedding layer that transforms the user ID into a dense vector representation. This approach reduces dimensionality while capturing user-specific features.\n",
    "\n",
    "Combine Features: Use user ID embeddings in conjunction with other features like user demographics, interaction history, and product attributes. This can create a more holistic view of user preferences.\n",
    "\n",
    "Regularization: Implement techniques like dropout or weight regularization to mitigate overfitting when using user IDs or their embeddings.\n",
    "\n",
    "Batch Normalization: Use batch normalization to stabilize learning, especially if the user ID leads to a wide range of outputs.\n",
    "----\n",
    "\"\"\"\n",
    "class RecommendationEnv(gym.Env):\n",
    "    def __init__(self, users:List[Customer], products:List[Product], top_k:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.users = users                  # list of users as states\n",
    "        self.products = products            # products as actions, potential recommendations\n",
    "        self.top_k = top_k                  # number of recommendations\n",
    "        self.user_idx = 0                   # index of users list, not user_id\n",
    "        self.current_step = 0               # step is also the interactions list index\n",
    "        self.categories = loader.load_categories()\n",
    "        \n",
    "        self.action_space = spaces.MultiDiscrete([len(products)] * 10) \n",
    "        \n",
    "        # number of customers as states\n",
    "        # states are derived from customer profiles and interactioms\n",
    "        # Users list will keep track of unique users\n",
    "        # States include subset of features including product, interaction, ratings, and time in one-hot-encoding format\n",
    "        # States exclude user_ids for policy network generalisation. But internal users list will be used as reference        \n",
    "        self.observation_space = spaces.Dict({\n",
    "            'pref_prod': spaces.Box(low=0, high=1, shape=(len(self.products),), dtype=np.float32),\n",
    "            'pref_cat': spaces.Box(low=0, high=1, shape=(len(self.categories),), dtype=np.float32),\n",
    "            'buys': spaces.Box(low=0, high=1, shape=(len(self.products),), dtype=np.uint8),\n",
    "            'views': spaces.Box(low=0, high=1, shape=(len(self.products),), dtype=np.uint8),\n",
    "            'likes': spaces.Box(low=0, high=1, shape=(len(self.products),), dtype=np.uint8),\n",
    "            'ratings': spaces.Box(low=0, high=1, shape=(len(self.products),), dtype=np.uint8),\n",
    "            'product': spaces.Box(low=0, high=1, shape=(len(self.products),), dtype=np.uint8),\n",
    "            'interaction': spaces.Box(low=0, high=1, shape=(len(list(InteractionType)),), dtype=np.uint8),\n",
    "            'rating': spaces.Discrete(6)\n",
    "            }) \n",
    "            ## add more features like time, ignored recommendtions, engagement etc\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        # Call the parent class's reset method to handle seeding\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        self.user_idx = np.random.randint(len(self.users)) # may run throught users one by one\n",
    "        user = self.users[self.user_idx]\n",
    "        self.current_step = 0\n",
    "        return self._get_observation(user), {}# get current user features as states\n",
    "\n",
    "    def step(self, rec_products):\n",
    "        \"\"\" randomly interacting with product to mimick real user unpredictable behavious \"\"\"\n",
    "        self.current_step += 1\n",
    "        user = self.users[self.user_idx]\n",
    "        \n",
    "        reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # simulate selected recommended product and interaction\n",
    "        seleted_pid, interaction_type = self._simulate_interaction(rec_products) # generate random interaction\n",
    "        # seleted_pid = random.choice(rec_products)\n",
    "        # interaction_type = random.choice(list(InteractionType))\n",
    "        \n",
    "        random_rating = 0\n",
    "        \n",
    "        if interaction_type == InteractionType.NONE:\n",
    "            reward = -1 # no interaction, customers not interested in recommendations\n",
    "        elif interaction_type ==  InteractionType.VIEW:\n",
    "            reward = 3\n",
    "        elif interaction_type ==  InteractionType.LIKE:\n",
    "            reward = 10\n",
    "        elif interaction_type ==  InteractionType.BUY:\n",
    "            reward = 50\n",
    "        elif interaction_type ==  InteractionType.RATE:\n",
    "            # generate rating, reward 1-2 is negative 3 neutral and 5 positive\n",
    "            random_rating = random.randint(0, 5)\n",
    "            reward = random_rating -1        \n",
    "        elif interaction_type ==  InteractionType.SESSION_START:\n",
    "            reward = 0\n",
    "        elif interaction_type ==  InteractionType.SESSION_CLOSE:\n",
    "            done = True\n",
    "            reward = 0 # TODO: check if engament is too short\n",
    "        else:\n",
    "            reward = 0\n",
    "        \n",
    "        # generate random interaction\n",
    "        new_interaction = Interaction(self.current_step, datetime.now(), user.idx, seleted_pid, interaction_type, random_rating)\n",
    "        # reward = self._calculate_reward(user, product)\n",
    "        \n",
    "        return self._update_observation(new_interaction), reward, done, False, {}\n",
    "\n",
    "    def _update_observation(self, interaction:Interaction):   \n",
    "        # update user data     \n",
    "        user = self.users[self.user_idx]   \n",
    "        pid = interaction.product_idx    \n",
    "         \n",
    "        if interaction == InteractionType.VIEW:\n",
    "            user.views[pid] += 1\n",
    "        elif interaction == InteractionType.LIKE:\n",
    "            user.likes[pid] += 1\n",
    "        elif interaction == InteractionType.BUY:\n",
    "            user.buys[pid] += 1\n",
    "        elif interaction == InteractionType.RATE:\n",
    "            user.rates[pid] = interaction.value\n",
    "          \n",
    "        # update observation based on new data  \n",
    "        obs = {\n",
    "                'pref_prod': self._get_product_preferences(user),\n",
    "                'pref_cat': self._get_category_preferences(user), \n",
    "                'buys': utils.normalise(user.buys),\n",
    "                'views': utils.normalise(user.views),\n",
    "                'likes': utils.normalise(user.likes),\n",
    "                'ratings': user.ratings,\n",
    "                'product': utils.one_hot_encode(pid, len(self.products)),\n",
    "                'interaction': self._get_interaction_observation(interaction),\n",
    "                'rating': interaction.value if interaction.type == InteractionType.RATE else 0 \n",
    "            }\n",
    "        \n",
    "        return obs\n",
    "\n",
    "    def _get_observation(self, user:Customer): \n",
    "        \n",
    "        obs = {\n",
    "                'pref_prod': self._get_product_preferences(user),\n",
    "                'pref_cat': self._get_category_preferences(user), \n",
    "                'buys': utils.normalise(user.buys),\n",
    "                'views': utils.normalise(user.views),\n",
    "                'likes': utils.normalise(user.likes),\n",
    "                'ratings': user.ratings,\n",
    "                'product': np.zeros(len(self.products)),\n",
    "                'interaction': np.zeros(len(list(InteractionType))),\n",
    "                'rating': 0 \n",
    "            }\n",
    "        \n",
    "        return obs\n",
    "        \n",
    "    def _get_interaction_observation(self, interaction:Interaction):\n",
    "        idx = list(InteractionType).index(interaction.type)\n",
    "        size = len(InteractionType)\n",
    "        \n",
    "        return utils.one_hot_encode(idx, size)\n",
    "    \n",
    "    # calculate preferences based on past interactions\n",
    "    def _get_product_preferences(self, user:Customer):\n",
    "        view_prefs = user.views / 20\n",
    "        purchase_prefs = user.buys\n",
    "        like_prefs = user.likes / 15\n",
    "\n",
    "        rating_prefs = user.ratings.copy()\n",
    "        rating_prefs[rating_prefs > 0] -= 2\n",
    "        \n",
    "        product_prefs = view_prefs + purchase_prefs + like_prefs+ rating_prefs\n",
    "        \n",
    "        return product_prefs    # calculate preferences based on past interactions\n",
    "    \n",
    "    def _get_category_preferences(self, user:Customer):\n",
    "        prod_prefs = self._get_product_preferences(user)\n",
    "        cat_prefs = np.zeros(len(self.categories), np.float32)\n",
    "        \n",
    "        for idx, prod_pref in enumerate(prod_prefs):\n",
    "            if prod_pref > 0:\n",
    "                product = self.products[idx]\n",
    "                cat_idx = product.category.idx\n",
    "                cat_prefs[cat_idx] += prod_pref # accumulation of fav products for this cat\n",
    "                # print(f\"added pf {prod_pref} to cat {cat_idx}\")\n",
    "                \n",
    "        cat_prefs = cat_prefs / 5 # reduce space     \n",
    "           \n",
    "        return cat_prefs\n",
    "\n",
    "    def _simulate_interaction(self, product_ids):        \n",
    "        user = self.users[self.user_idx]\n",
    "        product_list = []\n",
    "        \n",
    "        # simulate selection\n",
    "        num_products = len(product_ids)\n",
    "        prod_scores = np.zeros(num_products, np.uint8)\n",
    "        product_prefs = self._get_product_preferences(user)\n",
    "        category_prefs = self._get_category_preferences(user)\n",
    "        product_probs = np.full((num_products,), 1.0 / num_products) # equal probs by default\n",
    "        product_probs[-1] = 0.1 # lower ending epsidoe flag to encourage longer training\n",
    "        \n",
    "        for idx, pid in enumerate(product_ids):\n",
    "            product_list.append(self.products[pid]) # get the product objects\n",
    "            prod_scores[idx] = product_prefs[pid]\n",
    "            \n",
    "        # combining category prefs to calculate probabilities\n",
    "        for idx, product in enumerate(product_list):\n",
    "            cid = product.category.idx\n",
    "            prod_scores[idx] = category_prefs[cid] \n",
    "    \n",
    "        # Ensure the probabilities sum to 1 for a valid probability distribution\n",
    "        if np.argmax(prod_scores) > 0: # the product is in the preferences\n",
    "            product_probs = np.array(prod_scores) / sum(prod_scores)\n",
    "\n",
    "        # Randomly select a product based on the defined probabilities\n",
    "        selected_product_id = np.random.choice(product_ids, p=product_probs)\n",
    "        \n",
    "        # simulate interaction for the selected product\n",
    "        inter_types = list(InteractionType)\n",
    "        inter_scores = np.zeros(len(inter_types), np.uint8)\n",
    "        inter_probs = np.full((len(inter_types),), 1.0 / len(inter_types)) # equal probs by default\n",
    "        \n",
    "        \n",
    "        for idx, inter_type in enumerate(inter_types):\n",
    "            if inter_type == InteractionType.VIEW:\n",
    "                inter_scores[idx] = user.views[selected_product_id]\n",
    "            if inter_type == InteractionType.LIKE:\n",
    "                inter_scores[idx] = user.likes[selected_product_id]\n",
    "            if inter_type == InteractionType.BUY:\n",
    "                inter_scores[idx] = user.buys[selected_product_id]\n",
    "            if inter_type == InteractionType.RATE:\n",
    "                inter_scores[idx] = user.ratings[selected_product_id]\n",
    "        \n",
    "        if np.argmax(inter_scores) > 0:\n",
    "            inter_scores[inter_scores == 0] = 1 # default score for interaction that are 0\n",
    "            inter_probs = np.array(inter_scores) / sum(inter_scores)\n",
    "\n",
    "        # Randomly select a product based on the defined probabilities\n",
    "        selected_interaction_type = np.random.choice(inter_types, p=inter_probs)\n",
    "        \n",
    "        return selected_product_id, selected_interaction_type\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"\n",
    "        Set the seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if hasattr(self, 'last_action'):\n",
    "            print(f\"Recommended Product ID (Last Action): {self.last_action}\")\n",
    "        else:\n",
    "            print(\"No product recommended yet.\")\n",
    "\n",
    "        # Optionally, print the reward received for the last action\n",
    "        if hasattr(self, 'last_reward'):\n",
    "            print(f\"Reward for Last Action: {self.last_reward}\")\n",
    "\n",
    "        print(\"-----\")\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Load environment data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading products and customers from custom dataloaders in spark.data.loaders.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = loader.load_products()\n",
    "customers = loader.load_customers(include_interactions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising customer profiles from interactions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for customer in customers:\n",
    "    customer.views = np.zeros(len(products), dtype=np.int8)\n",
    "    customer.likes = np.zeros(len(products), dtype=np.int8)\n",
    "    customer.buys = np.zeros(len(products), dtype=np.int8)\n",
    "    customer.ratings = np.zeros(len(products), dtype=np.int8)\n",
    "    for interaction in customer.interactions:\n",
    "        i_type = interaction.type.value    \n",
    "        product_idx = interaction.product_idx  \n",
    "        # print(f\"customer {customer.idx} interaction {type} product {product_idx}\")\n",
    "        if i_type == InteractionType.VIEW.value:\n",
    "            customer.views[product_idx] += 1\n",
    "            # print(f\"customer {customer.idx} view\", customer.views)\n",
    "        elif i_type == InteractionType.LIKE.value:\n",
    "            customer.likes[product_idx] += 1\n",
    "            # print(f\"customer {customer.idx} like\", customer.likes)\n",
    "        elif i_type == InteractionType.BUY.value:\n",
    "            customer.buys[product_idx] += 1\n",
    "            # print(f\"customer {customer.idx} buy\", customer.buys)\n",
    "        elif i_type == InteractionType.RATE.value:\n",
    "            customer.ratings[product_idx] = interaction.value\n",
    "            # print(f\"customer {customer.idx} rate\", customer.rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4) Initialise training and evaluation environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training environment\n",
    "env = DummyVecEnv([lambda: Monitor(RecommendationEnv(customers, products, top_k=10))])\n",
    "env.seed(100)\n",
    "\n",
    "# Evaluation environment for WandbEvalCallback\n",
    "eval_env = DummyVecEnv([lambda: Monitor(RecommendationEnv(customers, products, top_k=10))])\n",
    "\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Custom Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction from frames as observations / states\n",
    "class CustomANN(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=128):\n",
    "        super(CustomANN, self).__init__(observation_space, features_dim)\n",
    "        \n",
    "        # Define your neural network layers\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(observation_space.shape[0], 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, features_dim)  # Output dimension should match features_dim\n",
    "        )\n",
    "    \n",
    "    def forward(self, observations):\n",
    "        return self.net(observations)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) Define Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 100`, after every 1 untruncated mini-batches, there will be a truncated mini-batch of size 36\n",
    "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
    "Info: (n_steps=100 and n_envs=1)\n",
    "  warnings.warn(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_clip_range = 0.2\n",
    "param_learning_rate = 0.0001\n",
    "param_gamma=0.995\n",
    "param_gae_lambda=0.95\n",
    "param_n_steps=100\n",
    "param_batch_size=50 # Set to 50 as it is a factor of 100\n",
    "param_n_epochs=10\n",
    "param_ent_coef = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    env=env,\n",
    "    policy='MultiInputPolicy',\n",
    "    verbose=0,\n",
    "    clip_range=param_clip_range,\n",
    "    learning_rate=param_learning_rate,\n",
    "    n_epochs=param_n_epochs,\n",
    "    n_steps=param_n_steps,\n",
    "    ent_coef=param_ent_coef,\n",
    "    batch_size=param_batch_size,\n",
    "    gamma=param_gamma,\n",
    "    gae_lambda=param_gae_lambda,\n",
    "    # policy_kwargs={'features_extractor_class': CustomANN},\n",
    "    tensorboard_log=logs_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logger for stdout, CSV, and TensorBoard\n",
    "new_logger = configure(str(logs_dir), [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# Model learning with WandbCallback and interim model saving\n",
    "model.learn(\n",
    "    total_timesteps=config[\"total_timesteps\"],\n",
    "    callback=WandbEvalCallback(\n",
    "                                eval_env=eval_env,\n",
    "                                wandb_run=run,\n",
    "                                save_interval=save_interval,\n",
    "                                eval_freq=save_interval,\n",
    "                                save_path=models_dir\n",
    "                            )\n",
    "    )\n",
    "\n",
    "# Save the final model after training completes\n",
    "final_model_path = os.path.join(models_dir, model_name_final)\n",
    "model.save(final_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2) Close out WandB session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "\n",
    "# Access attributes directly from the run object\n",
    "# or from the W&B App\n",
    "username = wandb.run.entity\n",
    "project = wandb.run.project\n",
    "run_id = wandb.run.id\n",
    "\n",
    "run = api.run(f\"{username}/{project}/{run_id}\")\n",
    "run.config[\"bar\"] = 32\n",
    "run.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1) Load tensorboard session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir={log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2) Outcome debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct observation data for customer 10\n",
    "test_customer = customers[10] \n",
    "\n",
    "# simulated laset selected product\n",
    "test_product = products[53]\n",
    "\n",
    "# it only uses the previous interaction to predict\n",
    "previous_interaction = Interaction(idx = '0', \n",
    "                          timestamp = datetime.now(), \n",
    "                          customer_idx = test_customer.idx, \n",
    "                          product_idx = test_product.idx, \n",
    "                          type = InteractionType.RATE,\n",
    "                          value = 5,)\n",
    "\n",
    "new_obs = env.update_observation(test_customer, interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_products, _states = model.predict(new_obs)\n",
    "print(recommended_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, done, flag, simulated_interaction_info = env.step(recommended_products)\n",
    "simulated_interaction_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for _ in range(20):  # Make 10 recommendations\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, _ = env.step(action)\n",
    "    print(\"Recommended products:\", action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
